{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistics Regression"
      ],
      "metadata": {
        "id": "Lg1KBX-s4xy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "p04Q4Vuy7NT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- Logistic Regression is a statistical model used for classification problems. It predicts the probability that a given input belongs to a particular class (usually binary: 0 or 1, True or False).\n",
        "The output is always between 0 and 1, representing the probability of class 1\n",
        "\n",
        "Linear Regression:\n",
        "- Purpose: \tPredict continuous values\n",
        "- Output Range: \t-∞ to +∞  \n",
        "- Equation:Linear combination of inputs\n",
        "- Use Case: \tRegression problems (e.g., house prices)\n",
        "- Loss Function: Mean Squared Error (MSE)\n",
        "- Interpretation of Output: \tDirect numerical prediction\n",
        "\n",
        "\tLogistic Regression\n",
        "- Purpose: Predict probabilities for categorical classes\n",
        "- Output Range: \tBetween 0 and 1\n",
        "- Equation: \tLogistic (sigmoid) transformation of linear function\n",
        "- Use Case: Classification problems (e.g., spam vs. not spam)\n",
        "- Loss Function: Log Loss / Binary Cross-Entropy\n",
        "- Interpretation of Output: Probability of class membership"
      ],
      "metadata": {
        "id": "2Qt3UBSI45pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "Dzm1l-XT7Sha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- The mathematical equation of logistic regression models the probability that the dependent variable\n",
        "𝑌\n",
        "Y equals 1 (success) given predictors\n",
        "𝑋\n",
        "X:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "Here:\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept.\n",
        "\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "  are coefficients for features\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        " .\n",
        "\n",
        "The sigmoid function ensures the output is between 0 and 1.\n",
        "\n",
        "Alternatively, in log-odds form:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n"
      ],
      "metadata": {
        "id": "sHULIFCY7Vfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "jx4JNjpC8R-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- We use the sigmoid function in logistic regression because it converts any real-valued number (from\n",
        "−\n",
        "∞ to\n",
        "+∞) into a value between 0 and 1, which can be interpreted as a probability.\n",
        "- Probability output: The target in classification is usually binary (0 or 1). The sigmoid maps the model's linear prediction to a probability.\n",
        "- Smooth and differentiable: Makes optimization with gradient descent feasible.\n",
        "- Interpretability: The output can be directly interpreted as the probability of belonging to class 1.\n",
        "- Thresholding: You can choose a cutoff (e.g., 0.5) to classify observations."
      ],
      "metadata": {
        "id": "LgmkPp4k8V4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "KHltfBh194yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- The cost function for logistic regression is the log loss (also called binary cross-entropy). It measures how well the predicted probabilities match the actual class labels (0 or 1).\n",
        "- The cost function of logistic regression is log loss or binary cross-entropy.\n",
        "- It penalizes wrong, confident predictions heavily.\n",
        "- It is convex, allowing efficient optimization."
      ],
      "metadata": {
        "id": "wx2gx2O-9-K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "2Q3XBikz-w_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function of logistic regression.\n",
        "- It discourages the model from fitting too closely to the training data by shrinking the coefficients.\n",
        "- It helps the model generalize better to new, unseen data.\n",
        "\n",
        "  why?\n",
        "- Prevents overfitting: Helps avoid overly complex models that memorize training data.\n",
        "- Improves generalization: Leads to better performance on new, unseen data.\n",
        "- Controls model complexity: Shrinks unnecessary or irrelevant coefficients.\n",
        "- Handles multicollinearity: Particularly with L2 regularization.\n",
        "\n"
      ],
      "metadata": {
        "id": "n04Kidnn-1Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression."
      ],
      "metadata": {
        "id": "bY1N-iNy_G6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-  1. Ridge Regression (L2 Regularization)\n",
        "Penalty: Adds the sum of squared coefficients to the cost function.\n",
        "\n",
        "Penalty term\n",
        "=\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty term=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect: Shrinks coefficients but does not make them exactly zero.\n",
        "\n",
        "Best for: When all predictors are useful and multicollinearity exists.\n",
        "\n",
        "2. Lasso Regression (L1 Regularization)\n",
        "Penalty: Adds the sum of absolute values of coefficients.\n",
        "\n",
        "Penalty term\n",
        "=\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Penalty term=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect: Can shrink some coefficients exactly to zero → performs feature selection.\n",
        "\n",
        "Best for: When you believe some features are irrelevant.\n",
        "\n",
        "3. Elastic Net Regression (Combination of L1 & L2)\n",
        "Penalty: Mix of both L1 and L2 regularization.\n",
        "\n",
        "Penalty term\n",
        "=\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty term=λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect: Combines feature selection (Lasso) and stability (Ridge).\n",
        "\n",
        "Best for: When there are many correlated features and some feature selection is needed."
      ],
      "metadata": {
        "id": "0fnHdw-2_Jca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n"
      ],
      "metadata": {
        "id": "h9HZSJ43_kjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- Elastic Net is best used when you have many features that are correlated with each other.\n",
        " Key reasons to choose Elastic Net:\n",
        "1. Correlated predictors:\n",
        "- Lasso tends to pick just one predictor from a group of correlated variables and ignore the rest.\n",
        "- Elastic Net can keep groups of correlated variables together.\n",
        "\n",
        "2️.Balances L1 and L2 penalties:\n",
        "- Combines Lasso’s feature selection (sparse solutions) with Ridge’s stability (handles multicollinearity).\n",
        "- You can tune the mix (using the l1_ratio parameter in many software packages).\n",
        "3.High-dimensional data (p > n):\n",
        "- When you have more features than observations, Elastic Net is more stable than Lasso alone.\n",
        "\n",
        "4️. Avoids Lasso limitations:\n",
        "- Pure Lasso can behave erratically when predictors are highly correlated.\n",
        "- Elastic Net regularizes in a more balanced way.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xp9AfRyJ_n1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?"
      ],
      "metadata": {
        "id": "-axKmo_xASkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "- The regularization parameter λ (lambda) controls how much penalty is applied to the model’s coefficients.\n",
        "- It determines the strength of regularization.\n",
        "- The regularization parameter λ in logistic regression controls the strength of penalty on coefficients. A higher λ shrinks coefficients more, reducing overfitting but risking underfitting. A lower λ allows more flexibility but may overfit. It helps balance model complexity and generalization."
      ],
      "metadata": {
        "id": "ixKqwGJ_AWDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "q57IBMmhtiI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-  \n",
        "- Binary or categorical outcome:\n",
        " - The dependent variable should be binary (0/1) or categorical (for multiclass logistic regression).\n",
        "- Independence of observations:\n",
        " - Each observation (row) in the dataset should be independent of the others.\n",
        " - Violations (e.g., repeated measures) can bias results.\n",
        "- Linearity of independent variables with log-odds:\n",
        " - Logistic regression does not assume linearity between predictors and the outcome,but it assumes a linear relationship between the predictors and the log-odds of the outcome.\n",
        "- No multicollinearity:\n",
        " - Predictors should not be highly correlated with each other.\n",
        " - High multicollinearity can distort the estimates of the coefficients."
      ],
      "metadata": {
        "id": "2L5-WLxhtmh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "rHkwO003uOY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-  1. Decision Trees\n",
        "- Idea: Split data into branches based on feature values.\n",
        "- Advantages: Easy to interpret, handles non-linear relationships.\n",
        "- Use case: When you want explainable models with simple rules.\n",
        "2. Random Forests\n",
        "- Idea: Ensemble of many decision trees, voting for the final class.\n",
        "- Advantages: Reduces overfitting, handles complex relationships.\n",
        "- Use case: High accuracy needed with low risk of overfitting.\n",
        "3. Support Vector Machines (SVM)\n",
        "- Idea: Finds the best boundary (hyperplane) that separates classes.\n",
        "- Advantages: Works well with high-dimensional data, robust to outliers.\n",
        "- Use case: When classes are well-separated or in text classification.\n",
        "4. k-Nearest Neighbors (k-NN)\n",
        "- Idea: Classifies based on the majority class among nearest neighbors.\n",
        "- Advantages: Simple, no training required.\n",
        "- Use case: Small datasets, non-linear boundaries.\n"
      ],
      "metadata": {
        "id": "kyL1tUshuq2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "qopmVV25vKOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- Classification evaluation metrics are measures used to assess how well a classification model performs on predicting class labels.\n",
        "- Accuracy: Proportion of correctly predicted observations.\n",
        "- Precision: Proportion of positive predictions that are actually correct.\n",
        "- Recall (Sensitivity, True Positive Rate): Proportion of actual positives correctly identified.\n",
        "- F1 Score: Harmonic mean of precision and recall.\n",
        "- ROC Curve and AUC (Area Under Curve)\n",
        " - ROC Curve: Plots TPR vs. FPR.\n",
        " - AUC: Measures overall ability to distinguish classes (higher = better).\n",
        "\n"
      ],
      "metadata": {
        "id": "DeM5qd1pvNDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "VltRjlguwZE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- Biased Predictions Toward Majority Class\n",
        "- Logistic regression tends to predict the majority class more often, ignoring the minority class.\n",
        "- A model might achieve high accuracy by always predicting the majority class — but it’s misleading.\n",
        "\n",
        "Poor Recall for Minority Class\n",
        "- The model may miss many true positives (high false negatives), resulting in low recall for the minority class.\n",
        "\n",
        "Misleading Accuracy Metric\n",
        "- Accuracy becomes an unreliable metric.\n",
        "- Example: A model predicting all \"0\" in a 95:5 dataset still gets 95% accuracy but 0% recall for class 1.\n",
        "\n",
        "Poor Probability Calibration\n",
        "- The predicted probabilities can be skewed, making threshold-based decisions less effective.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zmjb4NDpwccV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "5axCHiQtxhaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "- Hyperparameter tuning is the process of finding the best values for hyperparameters—settings that are not learned from the data directly, but control how the learning process works.\n",
        "- In logistic regression, these hyperparameters affect model complexity, regularization, and performance.\n",
        "- Hyperparameter tuning in logistic regression is the process of selecting the best values for settings like regularization strength (λ or C), penalty type (L1, L2, Elastic Net), and solver to improve model performance. It helps prevent overfitting or underfitting by finding the optimal complexity through methods like grid search and cross-validation.\n"
      ],
      "metadata": {
        "id": "2POsLphfxndk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "6XMR9nAwx-MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "- liblinear: Coordinate descent optimization (used in linear SVMs)\n",
        "- lbfgs: \tQuasi-Newton method (efficient gradient-based optimizer)\n",
        "- newton-cg: Newton’s method using conjugate gradients\n",
        "- saga: \tStochastic optimization for large-scale datasets\n",
        "- sag: Stochastic Average Gradient (faster on large datasets with many samples)"
      ],
      "metadata": {
        "id": "SdA374lKzAR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "uvw4XvAD0a5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "1. One-vs-Rest (OvR / One-vs-All):\n",
        "- Train one binary classifier per class.\n",
        "- Each classifier predicts this class vs. all others.\n",
        "- For prediction, pick the class with the highest probability.\n",
        " - Advantages: Simple, works with any binary classifier.\n",
        "  - Supported in: scikit-learn by default.\n",
        "2. Multinomial (Softmax) Logistic Regression:\n",
        "- Uses single model to predict all classes at once.\n",
        "3.  One-vs-One (less common in logistic regression):\n",
        "- Train classifiers for every pair of classes.\n",
        "- Prediction by majority vote among pairwise classifiers.\n",
        "\n"
      ],
      "metadata": {
        "id": "18rkUkBG0hF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "qIRr_Z2G089G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "Advantages of Logistic Regression\n",
        "1. Easy to understand and interpret:\n",
        "- Coefficients show the effect of each feature on the odds of the outcome.\n",
        "- Probabilistic output makes decisions transparent.\n",
        "2. Fast and efficient to train:\n",
        "- Works well on small to medium-sized datasets.\n",
        "- Computationally inexpensive.\n",
        "3. Good baseline model:\n",
        "- Often performs surprisingly well on linearly separable data.\n",
        "- Easy to benchmark against more complex models.\n",
        "4. Outputs probabilities:\n",
        "- Provides probability estimates instead of just class labels.\n",
        "- Useful for ranking and risk scoring."
      ],
      "metadata": {
        "id": "pXGkBLzA1A9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "Lm8UJRQj1tOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        " Medical Diagnosis:\n",
        "- Predict whether a patient has a disease (yes/no) based on features like age, test results, symptoms.\n",
        "-  Example: Diabetes prediction.\n",
        "\n",
        "Credit Scoring / Risk Assessment:\n",
        "- Predict if a borrower will default on a loan.\n",
        "- Example: Bank uses applicant income, employment status, credit history.\n",
        "\n",
        "Marketing and Customer Conversion:\n",
        "- Predict if a customer will buy a product after seeing an ad.\n",
        "- Example: Ad click prediction (click/no click).\n",
        "\n",
        "Spam Detection:\n",
        "- Classify emails as spam or not spam based on text features.\n",
        "-  Example: Email service filters.\n",
        "\n"
      ],
      "metadata": {
        "id": "YizZcyyK1wVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "aWPJZFd82ahs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "Logistic Regression\n",
        "- Purpose: \tBinary classification (2 classes)\n",
        "- Output: \tProbability of one class (e.g., class 1 vs class 0)\n",
        "- Activation Function: \tSigmoid function\n",
        "- Model Output: \tOne probability (others = 1 − predicted prob)\n",
        "- Use Case Example: \tSpam (Yes/No), loan default (Yes/No)\n",
        "\n",
        "\tSoftmax Regression (Multinomial Logistic Regression)\n",
        "- Purpose: \tMulticlass classification (3 or more classes)\n",
        "- Output: Probability distribution over all classes\n",
        "- Activation Function: \tSoftmax function\n",
        "- Model Output: Probabilities for each class summing to 1\n",
        "- Use Case Example: Classifying animal images into cat, dog, or bird\n"
      ],
      "metadata": {
        "id": "cBhbipqV2gJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "tu57d9Oo34r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- One-vs-Rest (OvR) Approach\n",
        "- Train one binary classifier for each class: this class vs all others.\n",
        "- For prediction, pick the class with the highest probability.\n",
        "\n",
        "Advantages:\n",
        "- Simpler, easy to implement.\n",
        "- Works even if your base logistic regression model only supports binary.\n",
        "- Can be more robust if classes are imbalanced or overlapping.\n",
        "\n",
        "When to Use OvR:\n",
        "- Small datasets.\n",
        "- When interpretability is important.\n",
        "- When you suspect some classes are hard to separate cleanly from all others.\n",
        "\n",
        "Softmax (Multinomial) Logistic Regression\n",
        "- Uses one single model to handle all classes at once.\n",
        "- Outputs probability distribution over all classes (they sum to 1).\n",
        "\n",
        "Advantages:\n",
        "- Naturally handles mutually exclusive classes.\n",
        "- Usually better overall calibration of probabilities.\n",
        "- Often performs better on well-separated classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "cTWpG9et38QW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "NaM1Gn2t443m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "- In logistic regression, the coefficients represent the relationship between the predictor variables and the log-odds of the target class.\n",
        "- For a coefficient\n",
        "𝛽\n",
        "𝑗\n",
        "associated with a predictor\n",
        "𝑥\n",
        "𝑗\n",
        " - A 1-unit increase in\n",
        "𝑥\n",
        "𝑗\n",
        "  changes the log-odds of the outcome by\n",
        "𝛽\n",
        "𝑗\n",
        ", assuming all other variables remain constant.\n",
        "\n",
        " - To interpret it more intuitively, we exponentiate the coefficient:\n",
        "                                   Odds Ratio=𝑒𝛽𝑗\n",
        "\n",
        "  - If 𝑒𝛽𝑗>1, the odds of the positive class increase with\n",
        "𝑥\n",
        "𝑗\n",
        "\n",
        " - If 𝑒𝛽𝑗<1, the odds of the positive class decrease with\n",
        "𝑥\n",
        "𝑗\n",
        "\n",
        " - If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗=1,\n",
        "𝑥\n",
        "𝑗\n",
        "has no effect on the odds."
      ],
      "metadata": {
        "id": "6bFzmly-49b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pratical"
      ],
      "metadata": {
        "id": "KGPbKm1a6uVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n"
      ],
      "metadata": {
        "id": "vMTjkNob9oBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQtHIDxO9pjg",
        "outputId": "cafa29e7-f899-465b-d8ff-a6d46668d2f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "_lJWlU0s9yzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=500, multi_class='multinomial')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jtO30rs911k",
        "outputId": "1d6e9ab7-7710-4262-c18d-78563640d358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "vTNXL1oB9_rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='multinomial')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Print intercepts\n",
        "print(\"Intercepts:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAipwv0S-C2b",
        "outputId": "bc977969-4d8d-415c-9094-2e2412f2d74d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "Intercepts:\n",
            "[  9.00884295   1.86902164 -10.87786459]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "KWulBSPW-Jf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,         # 50% L1, 50% L2\n",
        "    max_iter=500,\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Print intercepts\n",
        "print(\"Intercepts:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2RnmS7A-MKk",
        "outputId": "14085f7a-69d0-4f65-f024-ce5dffe72d1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.25877113  1.7365313  -2.43152374 -0.61936719]\n",
            " [ 0.          0.          0.         -0.50795359]\n",
            " [-1.01226685 -1.21085551  2.64666169  2.10759953]]\n",
            "Intercepts:\n",
            "[ 2.52064608  2.48582736 -5.00647344]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "f3mHSqbg-UWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy (One-vs-Rest): {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Print intercepts\n",
        "print(\"Intercepts:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqKtkegl-Xe6",
        "outputId": "1adeeae7-da85-4c97-adb9-5011bc256442"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (One-vs-Rest): 0.97\n",
            "Model Coefficients:\n",
            "[[-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            " [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            " [-0.38904645 -0.62147609  2.7762982   2.09067085]]\n",
            "Intercepts:\n",
            "[  6.24415327   4.81099217 -12.83530153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "dXpfcL3J-ggV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear', multi_class='ovr', max_iter=200)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=logreg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print best cross-validation accuracy\n",
        "print(f\"Best CV Accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbFfaXmz-jIP",
        "outputId": "56f3a4e3-d3b0-41bd-9464-4a18dd467498"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best CV Accuracy: 0.96\n",
            "Test Set Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "eDTDSgMe-sKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create model\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
        "\n",
        "# Store accuracy scores\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), start=1):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Fold {fold} Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"\\nAverage Accuracy: {np.mean(accuracies):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgojNYVV-vWT",
        "outputId": "2f1d8c51-1009-4a7f-dbcc-e11d0302061f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Accuracy: 1.00\n",
            "Fold 2 Accuracy: 0.97\n",
            "Fold 3 Accuracy: 0.93\n",
            "Fold 4 Accuracy: 1.00\n",
            "Fold 5 Accuracy: 0.93\n",
            "\n",
            "Average Accuracy: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "RDLPCrSz-1zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV\n",
        "# Replace \"your_data.csv\" with the actual path to your CSV file\n",
        "data = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Step 2: Define features (X) and target (y)\n",
        "X = data.drop(columns=[\"target\"])     # Replace 'target' with your label column name\n",
        "y = data[\"target\"]\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "_HO1MSju_C64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "yM2-KaQk_0G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter distributions (only valid combinations for solvers and penalties)\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 2, 10),  # C values from 0.001 to 100\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # These support both l1 and l2\n",
        "}\n",
        "\n",
        "# Create base model\n",
        "model = LogisticRegression(max_iter=500, multi_class='auto')\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,            # number of random combinations to try\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best cross-validation score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Best CV Accuracy: {random_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMBybyqf_4m0",
        "outputId": "bc1e4472-e0db-48a1-ae88-2affc74c4c8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': np.float64(2.1544346900318843)}\n",
            "Best CV Accuracy: 0.97\n",
            "Test Set Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "Zpmw-O52ABjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create base Logistic Regression model\n",
        "base_model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap in One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the OvO model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79vzDNTrAD5w",
        "outputId": "1e144901-4d2b-4345-d6bd-182dcbec5e85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "ICAo4x9TAPYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load a binary classification dataset (Breast Cancer)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (Binary Classification)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "_Kz1gJNjARjs",
        "outputId": "8e814620-ff45-4ca1-85ea-9db2513434b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Scor."
      ],
      "metadata": {
        "id": "gVNDIfKpAaXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy:  {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S6su84vAcT1",
        "outputId": "11d845b8-7c32-4ae2-e6a3-7cdda09bdfc0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.96\n",
            "Precision: 0.95\n",
            "Recall:    0.99\n",
            "F1-Score:  0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "YlCA3-GEAoHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Create an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],   # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Class distribution:\", {label: sum(y == label) for label in set(y)})\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=200)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Performance WITHOUT Class Weights ===\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Step 4: Train Logistic Regression with class_weight='balanced'\n",
        "model_balanced = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model_balanced.fit(X_train, y_train)\n",
        "y_pred_balanced = model_balanced.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Performance WITH Class Weights (Balanced) ===\")\n",
        "print(classification_report(y_test, y_pred_balanced))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCSOBIPeAqVx",
        "outputId": "2e4ac0f0-7860-42b6-a6ce-78dea2ffe600"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: {np.int64(0): np.int64(900), np.int64(1): np.int64(100)}\n",
            "\n",
            "=== Performance WITHOUT Class Weights ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96       180\n",
            "           1       0.78      0.35      0.48        20\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.85      0.67      0.72       200\n",
            "weighted avg       0.92      0.93      0.91       200\n",
            "\n",
            "\n",
            "=== Performance WITH Class Weights (Balanced) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.82      0.90       180\n",
            "           1       0.36      0.90      0.51        20\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.67      0.86      0.71       200\n",
            "weighted avg       0.92      0.83      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "EXMzioaFAy3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "print(titanic.head())\n",
        "\n",
        "# Step 2: Basic preprocessing\n",
        "# Drop rows where target ('survived') is missing\n",
        "titanic = titanic.dropna(subset=['survived'])\n",
        "\n",
        "# Fill missing age with median\n",
        "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
        "\n",
        "# Fill embarked with mode\n",
        "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'adult_male'])\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'alone'], drop_first=True)\n",
        "\n",
        "# Step 4: Define features and target\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate performance\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_V3gRI1A1Jd",
        "outputId": "362bc12d-49a4-4e33-f465-a3921503a2ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       110\n",
            "           1       0.80      0.68      0.73        69\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.79      0.79       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "Inhh-M2sA-jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ----------- Without Scaling -----------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "\n",
        "# ----------- With Standardization -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0kjiw_6BAyG",
        "outputId": "51c0e4dd-3826-43eb-9845-c1ed6a795cd2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.97\n",
            "Accuracy with scaling:    0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "5KOQ3VmcBGiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get predicted probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyXhuMYDBJuY",
        "outputId": "cc3afb34-35b5-44f1-81bd-c20f873c9da7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "W-MEz4GkBS1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with custom regularization strength (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1AwLthBWfG",
        "outputId": "85d111c8-cba9-4746-a532-626430f7db4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with C=0.5: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "RUSNixgGBeFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200, multi_class='ovr')  # Use 'ovr' to get separate coefficients for each class\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get feature importances (coefficients)\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Step 5: Display feature importance for each class\n",
        "print(\"Feature Importances (Logistic Regression Coefficients):\\n\")\n",
        "for class_idx, class_name in enumerate(iris.target_names):\n",
        "    print(f\"Class: {class_name}\")\n",
        "    feature_importance = pd.Series(coefficients[class_idx], index=X.columns)\n",
        "    print(feature_importance.sort_values(ascending=False), \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2cscKO-Bg-u",
        "outputId": "6bbf763a-981e-429a-a5aa-cb2c4504ef20"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Logistic Regression Coefficients):\n",
            "\n",
            "Class: setosa\n",
            "sepal width (cm)     0.887719\n",
            "sepal length (cm)   -0.427622\n",
            "petal width (cm)    -0.916100\n",
            "petal length (cm)   -2.214717\n",
            "dtype: float64 \n",
            "\n",
            "Class: versicolor\n",
            "petal length (cm)    0.542660\n",
            "sepal length (cm)   -0.033878\n",
            "petal width (cm)    -1.017937\n",
            "sepal width (cm)    -2.044299\n",
            "dtype: float64 \n",
            "\n",
            "Class: virginica\n",
            "petal length (cm)    2.776298\n",
            "petal width (cm)     2.090671\n",
            "sepal length (cm)   -0.389046\n",
            "sepal width (cm)    -0.621476\n",
            "dtype: float64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "nrZgDL12BoWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hC2ZCwhBq6O",
        "outputId": "bc2c7e9b-86f0-4a10-96e5-b329aae3acd5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:\n",
        "."
      ],
      "metadata": {
        "id": "eD1Io9aUByck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get predicted probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute precision-recall pairs\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Step 6: Compute average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "print(f\"Average Precision Score: {avg_precision:.2f}\")\n",
        "\n",
        "# Step 7: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "-Up2gSecB2Xg",
        "outputId": "e56a20eb-e8d9-4654-eab8-fbb52973c7f5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision Score: 1.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "fVEYK3Z7CAaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train and evaluate model with each solver\n",
        "print(\"Solver Comparison:\")\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500, multi_class='ovr')\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Solver: {solver:10s} | Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2GVXGXXCDfd",
        "outputId": "56df7bc0-7945-4e24-d36b-507d76ad6b5e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver Comparison:\n",
            "Solver: liblinear  | Accuracy: 0.97\n",
            "Solver: saga       | Accuracy: 0.90\n",
            "Solver: lbfgs      | Accuracy: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "FPaO-La5CJ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axod2W97CM7c",
        "outputId": "e5e5322c-c395-4f61-b8a0-5191d2f8d83b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "DSVMC_3XCUIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------- Train on RAW (unscaled) data ----------\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy without scaling: {accuracy_raw:.2f}\")\n",
        "\n",
        "# ---------- Train on STANDARDIZED (scaled) data ----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew5ZxlxjCaA0",
        "outputId": "dcb238b6-4b4b-4595-b7bd-f274e7e07382"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.97\n",
            "Accuracy with scaling:    0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "iE6z_ILFCh1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Define model and hyperparameter grid\n",
        "model = LogisticRegression(max_iter=500, multi_class='ovr')\n",
        "param_grid = {'C': np.logspace(-3, 3, 7)}  # e.g., [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "# Step 4: Perform GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Best parameter\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C (Regularization Strength): {best_C}\")\n",
        "\n",
        "# Step 6: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy with Best C: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpS6gH75Ck7W",
        "outputId": "94ec321d-7810-4b81-81c6-7d04480dda39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C (Regularization Strength): 10.0\n",
            "Test Set Accuracy with Best C: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "_NxlAm2kCvnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # For saving and loading models\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model to a file\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "print(\"Model saved to 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 5: Load the model from the file\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "print(\"Model loaded from 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 6: Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I32Wbp8CyuW",
        "outputId": "7f6ac26e-1c9d-4a84-8bb4-5b1b82ca114b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'logistic_model.joblib'\n",
            "Model loaded from 'logistic_model.joblib'\n",
            "Accuracy of loaded model: 0.97\n"
          ]
        }
      ]
    }
  ]
}